---
title: "How to load data into motherduck"
---


```{r}
#| label: setup
#| echo: false
#| eval: true
#| warning: false
#| message: false
#| include: true

library(md)
library(tidyverse)
load_all()
```


## Introduction

Before we get into how to use the package, lets quickly review the three things you will need to load data into a database:
-   Database name (Catalog Name)
    -   This is object that will hold your schemas, tables or views
    -   Different databases have their own naming convention, in motherduck, this is called Catalog 
-   Schema name
    -   This fancy name for the location that classifies and organizes your tables, functions, procedures, etc
-   Table or view name
    -   This is the name of the actual table or view that holds your data
    -   Table is a physical table of data that persists whereas a view is a stored procedures that queries underlying tables when needed
    
To save or reference data, you need to either fully qualify the name with `database_name.schema_name.table_name` or you need to be "in" your database and schema and reference the table name.

If you uploaded data without creating a table or schema first then duckdb will assign "temp" and "main" as the default names for your database and schema respectively.

::: {.callout-note}
## Duckdb vs. Motherduck

While not technically correct, motherduck is a cloud based deployment of motherduck where you can have multiple databases, control access / permissions, and scale compute / storage as needed

Duckdb is a essentially single database instance in your local computer where you can save the database to a file or create it in memory. 

If you have a local file or in memory duckdb connection, then you can't create a new database but this function will still work but it will just ignore the database_name you supply it and instead take the name of your current connection.

This is because a your local file / in memory connection is the database -- you can't create or replace a database on top of that. However in motherduck you can create multiple databases.

Through out these write ups, I tend to use duckdb & motherduck interchangeably however some functions only work with motherduck and this is motherduck focused package however most functions can work with local instances of duckdb

:::

## Let's upload some data

Later on we will show examples of how to read data from a source file, eg. csv, parquet, or even excel directly into motherduck without loading the data into memory but for now let's assume you want to upload some data that you already have loaded in your R environment.

First let us connect to our motherduck database. In order to connect, you will need to install and load the the motherduck extension from the community store. 

The `connect_to_motherduck()` function will take your token that is your environment file, install and load the extensions if needed and connect to your motherduck instance.


::: {.callout-caution}
## connecto-to-motherduck

One limitation of the connecting to motherduck from R is that you first need to create a local motherduck instance which then allows the connection to motherduck which means you have access to both local (temporary) duckdb database and your cloud based motherduck databases. 
:::


```{r}
#| label: connect
#| echo: true
#| eval: true
#| warning: true
#| message: true
#| include: true

con_md <- connect_to_motherduck(
  motherduck_token = "MOTHERDUCK_TOKEN"  #<1>
  )
```
1. Pass your token name from your R environment file

You will get a message that prints out that actions the package took and information about your connection

We can immediately check what database, schemas or tables we have access to with the `list_all_tables()` function

```{r}
#| label: list-all
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| include: true

list_all_tables(con_md)
```

Before uploading new data, it can be helpful to check "where" you are in your database

You can do this with the `pwd()`^[Naming convention is inspired by linux commands] function that will print out the current database & schema that you are in. 

This would be the default location that you save your database unless you clarified a different database and schema.


```{r}
#| label: pwd
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| include: true

pwd(con_md)
```
By default, you will be in your local duckdb database even though you are connected to motherduck

If we want to we can also navigate to a your motherduck database with the `cd()`command

```{r}
#| label: cd
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true

cd(con_md,database = "contoso")
```

For this example, we will want to create a new database and schema location to upload some existing data.  `create_table()` function will create a new database and save the table under the schema.


```{r}
#| label: create-table
#| echo: true
#| eval: true
#| warning: true
#| message: true
#| include: true

ggplot2::diamonds |>  #<1>
    md::create_table(
        .con = con_md #<2>
        ,database_name = "vignette" #<3>
        ,schema_name = "raw" #<4>
        ,table_name = "diamonds" #<5>
        ,write_type="overwrite"  #<6>
        )
```


1. Pass your data into the function
2. List your motherduck connection
3. database name (either new or existing)
4. schema name (either new or existing)
5. table name 
6. Either overwrite or append the data

Notice that we don't assign this object to anything, this just silently writes our data to our database and prints a message confirming the performed actions

To validate the data is in our database, we can do the following:

We can validate if we have successfully saved the table in our database by running `list_all_tables()`.



```{r}
#| label: list-tables-example
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| include: true

list_all_tables(con_md) |> 
  filter(
    table_catalog=="vignette"
  )
```

After running these functions, we can see our table is ready to be queried.

If you want to access your motherduck data, you can simply leverage` dplyr::tbl()` function with your motherduck connection to pull your data and from there leverage the fantastic dbplyr or dplyr package respectively to use tidy verbs to perform additional functions.

## organizing our data

Let's say we want to filter and summarize this table and save to our database with a new tablename -- no problem, we can repeat the steps and this time we will upload a DBI object instead of tibble.


```{r}
#| label: organize
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true


id_name <- DBI::Id("vignette","raw","diamonds") #<1>

diamonds_summary_tbl <- tbl(con_md,id_name) |>  #<2>
    summarise(
        .by=c(color,cut,clarity)
        ,mean_price=mean(price,na.rm=TRUE)
    )

diamonds_summary_tbl |> #<3>
    create_table(   
    .con = con_md
    ,database_name = "vignette"
    ,schema_name = "raw"
    ,table_name = "diamonds_summary" 
    ,write_type = "overwrite"
)
```
1. You can directly call the full name or if you are already in your database / schema you can just call the table
2. Perform your additional cleaning, transformation, or summarization steps
3. Pass the DBI object to create_table and it will still save the table!

While its the same syntax, `create_table()` will work with an R object or DBI object to save that table into your database. 


## create new schema

Let's saw we want to organize these existing tables into different schema, we can do this by first creating a new schema and then moving that table or alternatively loading a table directly with a new schema and table 


```{r}
#| label: new-schema
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true

    create_schema(
        .con=con_md
        ,database_name = "vignette"
        ,schema_name = "curated"
    )
```

This will create a new schema if it doesn't exist but won't load any data.

```{r}
#| label: list-schemas
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| include: true



list_schemas(con_md)

```
::: {.callout-note}
## default schemas
Remember as default your duckdb will create a `main` schema even if you don't save any table to it
If you are using duckdb then your database will default to "memory" or the file-path that you selected

Also note that in duckdb you can't delete main schema either
:::

We can copy one of tables to our schema with 


```{r}
#| label: copy-table-to-location
#| echo: true
#| eval: false
#| warning: false
#| message: true
#| include: true




md::copy_tables_to_new_location(
    .con = con_md
    ,from_table_names = "diamonds_summary"
    ,to_database_name = "vignette"
    ,to_schema_name = "raw"
    )
```

There's a complimentary function called `create_or_repalce_schema` which will also create a schema the different is if there is already a schema with that name it will delete that schema and any tables saved under it.

```{r}
list_schemas(con_md)
```

## drop databaes, schemas or tables

Sometimes we need to delete databases, schemas or tables. 

Be careful when you do this as its irreversible -- there is no CTRL+Z to undo this. 

::: {.callout-note}
## sql commands

Behind the scenes, the code is just executing the following SQL commands


```{sql}
#--eval: false

if exists drop `table_name` cascade

```

:::


```{r}
#| label: var
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true

delete_schema(con_md,database_name = "vignette",schema_name = "raw",cascade = TRUE)
```

```{r}
#| label: list-all-tables-1
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true
list_all_tables(con_md)
```


## how to load data directly into motherduck

For csv files we can leverage the existing duckdb function `duckdb::read_csv_duckdb()` to directly read the a csv file or a series of csv files^[as long as they have the same header structure] into your duckdb or motherduck database

This will read the files from their source location directly into your database without loading the files into memory which is helpful when you are dealing with larger than memory data.

Underneath the hood the duckdb function is using the `read_csv_auto` and you can pass the configuration options  directly through the the read_csv function if you need configuration.



```{r}
#| label: read-csv
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true

write.csv(mtcars,"mtcars.csv")

# cd(schema = "raw")

duckdb::duckdb_read_csv(conn = con_md,files = "mtcars.csv",name = "mtcars")

fs::file_delete("mtcars.csv")
```

For or excel, parquet or httpfs file formats, we can leverage md read_excel_duckdb, read_parquet_duckdb() or read_httpfs_duckdb() form the `md` package.

Similar to the `read_csv_auto` function, these leverage underlying duckdb extensions to read these diffrent file formatas.

You can view the default configuration tables with the md::config_* family of tables 



```{r}
#| label: read-xl
#| echo: true
#| eval: true
#| warning: false
#| message: true
#| include: true
    
openxlsx::write.xlsx(starwars,"starwars.xlsx")

# temp_con <- DBI::dbConnect(duckdb::duckdb())

read_excel_duckdb(
    .con=con_md
    ,to_database_name = "vignette"
    ,to_schema_name = "main"
    ,to_table_name = "starwars"
    ,file_path = "starwars.xlsx"
    ,header = TRUE
    ,sheet = "Sheet 1"
    ,range="automatically inferred"
    ,all_varchar  = TRUE
)

fs::file_delete("starwars.xlsx")

tbl(temp_con,"starwars")
```

We can add the following options directly to function to help with uploading the excel files. Note these are available in the documentation and via the build in datasets



```{r}

  
arrow::write_parquet(x = iris,sink = "iris.parquet")

read_parquet_duckdb(temp_con,to_table_name = "iris",file_path =  "iris.parquet")
usethis::use_data_raw("config_parquet")

list_all_tables(temp_con)

md::config_parquet
```


Underneath the hood, these functions will install and load the required duckdb extension and then go through the standard workflow of creating a table (and associated database / schema if not created) for your work


tabset -- configuration options 

## config options

```{r}

config_parquet <- tribble(
    ~name,               ~description,                                                                                 ~type,   ~default,
    "binary_as_string",  "Parquet files generated by legacy writers do not correctly set the UTF8 flag for strings, causing string columns to be loaded as BLOB instead. Set this to true to load binary columns as strings.", "BOOL", "false",
    "encryption_config", "Configuration for Parquet encryption.",                                                     "STRUCT", "-",
    "filename",          "Whether or not an extra filename column should be included in the result. Since DuckDB v1.3.0, the filename column is added automatically as a virtual column and this option is only kept for compatibility reasons.", "BOOL", "false",
    "file_row_number",   "Whether or not to include the file_row_number column.",                                       "BOOL", "false",
    "hive_partitioning", "Whether or not to interpret the path as a Hive partitioned path.",                          "BOOL", "(auto-detected)",
    "union_by_name",     "Whether the columns of multiple schemas should be unified by name, rather than by position.", "BOOL", "false"
)

config_parquet
```

```{r}

config_excel <- tribble(
    ~option,            ~type,     ~default,                  ~description,
    "header",           "BOOLEAN", "automatically inferred",  "Whether to treat the first row as containing the names of the resulting columns.",
    "sheet",            "VARCHAR", "automatically inferred",  "The name of the sheet in the xlsx file to read. Default is the first sheet.",
    "all_varchar",      "BOOLEAN", "false",                   "Whether to read all cells as containing VARCHARs.",
    "ignore_errors",    "BOOLEAN", "false",                   "Whether to ignore errors and silently replace cells that can't be cast to the corresponding inferred column type with NULLs.",
    "range",            "VARCHAR", "automatically inferred",  "The range of cells to read, in spreadsheet notation. For example, A1:B2 reads the cells from A1 to B2. If not specified, the resulting range will be inferred as the rectangular region of cells between the first row of consecutive non-empty cells and the first empty row spanning the same columns.",
    "stop_at_empty",    "BOOLEAN", "automatically inferred",  "Whether to stop reading the file when an empty row is encountered. If an explicit range option is provided, this is false by default; otherwise, true.",
    "empty_as_varchar", "BOOLEAN", "false",                   "Whether to treat empty cells as VARCHAR instead of DOUBLE when trying to automatically infer column types."
)
```




```{r}



md_con <- connect_to_motherduck(config=list(autoload_known_extensions="true"))
Sys.setenv(MOTHERDUCK_TOKEN = "your_motherduck_token_here")

# Connect to DuckDB in memory only (no local file created)
con <- dbConnect(
  duckdb::duckdb(config=list(autoload_known_extensions="true"))
  ,dbdir = ":md"
  ,motherduck_token=Sys.setenv(MOTHERDUCK_TOKEN = "your_motherduck_token_here")
  )

# Install and load the Motherduck extension
dbExecute(con, "INSTALL motherduck;")
dbExecute(con, "LOAD motherduck;")

# Attach Motherduck (empty catalog = default)
dbExecute(md_con, "ATTACH 'md:' AS md;")

# Now query something from your MD environment
dbGetQuery(con, "SELECT catalog_name FROM schemata;")

DBI::dbDisconnect(md_con)


con <- dbConnect(duckdb::duckdb(), dbdir = "md:",motherduck_token=Sys.getenv( "MOTHERDUCK_TOKEN"))

dbGetQuery(con, "SELECT table_schema, table_name FROM information_schema.tables;")


dbExecute(con, "ATTACH 'md:';")

dbExecute(md_con, "ATTACH 'md:';")
   

   
```

